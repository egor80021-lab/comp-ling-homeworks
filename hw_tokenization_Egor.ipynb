{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXdJjBa9sCq"
      },
      "source": [
        "# Домашнее задание: Токенизация текста"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дан список текстов, которые нужно токенизировать разными способами"
      ],
      "metadata": {
        "id": "1xVbvaj_phyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "\"The quick brown fox jumps over the lazy dog. It's a beautiful day!\",\n",
        "\"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\",\n",
        "\"I can't believe she's going! Let's meet at Jane's house. They'll love it.\",\n",
        "\"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "]"
      ],
      "metadata": {
        "id": "uj-xaNnwpiPo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Используйте способы токенизации, которые описаны в практикуме. Каждый способ нужно обернуть в функцию, например:\n",
        "\n",
        " ```python\n",
        " def simple_tokenization(string):\n",
        "   return string.split()\n",
        "   ```"
      ],
      "metadata": {
        "id": "ix1Im4Kcqb3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Напишите функцию для токенизации по пробелам и знакам препинания (используйте оператор `def`)"
      ],
      "metadata": {
        "id": "Ih0BBOGBpv6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re # добавим модуль регулярных выражений\n",
        "\n",
        "def simple_tokenization(string): # создадим функцию, параметр которой будет строка\n",
        "    \"\"\"\n",
        "    Токенизация: слова и знаки препинания как отдельные токены.\n",
        "    \"\"\"\n",
        "    return re.findall(r\"\\w+|[^\\w\\s]\", string) # возвращает список токенов. Условие – любой символ, который НЕ буква/цифра/подчёркивание и НЕ пробел\n",
        "\n",
        "# здесь же сразу вызываем функцию для ВСЕХ строк из text\n",
        "for i, sentence in enumerate(text, start=1):\n",
        "    print(f\"\\n=== Текст {i} ===\")\n",
        "    print(sentence)\n",
        "    print(simple_tokenization(sentence))"
      ],
      "metadata": {
        "id": "W1QCaw6cqDnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57153f7-59bf-4e99-d446-311a2dc073a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Текст 1 ===\n",
            "The quick brown fox jumps over the lazy dog. It's a beautiful day!\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'\", 's', 'a', 'beautiful', 'day', '!']\n",
            "\n",
            "=== Текст 2 ===\n",
            "Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\n",
            "['Dr', '.', 'Smith', 'arrived', 'at', '5', ':', '30', 'p', '.', 'm', '.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1', ',', '000', '.', '50', '.']\n",
            "\n",
            "=== Текст 3 ===\n",
            "I can't believe she's going! Let's meet at Jane's house. They'll love it.\n",
            "['I', 'can', \"'\", 't', 'believe', 'she', \"'\", 's', 'going', '!', 'Let', \"'\", 's', 'meet', 'at', 'Jane', \"'\", 's', 'house', '.', 'They', \"'\", 'll', 'love', 'it', '.']\n",
            "\n",
            "=== Текст 4 ===\n",
            "What's the ETA for the package? Please e-mail support@example.com ASAP!\n",
            "['What', \"'\", 's', 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support', '@', 'example', '.', 'com', 'ASAP', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Напишите функцию для токенизации текста с помощью NLTK"
      ],
      "metadata": {
        "id": "GThvPcovqgO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk                      # подключаем библиотеку NLTK\n",
        "from nltk.tokenize import word_tokenize  # берём функцию токенизации по словам\n",
        "\n",
        "# скачиваем данные для токенизации\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def nltk_tokenization(string):\n",
        "    \"\"\"\n",
        "    Токенизация текста с помощью NLTK (word_tokenize).\n",
        "    На вход: ОДНА строка.\n",
        "    На выход: список токенов (слов и знаков препинания).\n",
        "    \"\"\"\n",
        "    return word_tokenize(string)\n",
        "\n",
        "# ниже сразу применяем функцию ко всем предложениям из списка text\n",
        "for i, sentence in enumerate(text, start=1):\n",
        "    print(f\"\\n=== Текст {i} ===\")\n",
        "    print(sentence)\n",
        "    print(nltk_tokenization(sentence))"
      ],
      "metadata": {
        "id": "14BIv33iqrkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f20c8409-9a43-408d-e28c-2ec9f099e7d4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Текст 1 ===\n",
            "The quick brown fox jumps over the lazy dog. It's a beautiful day!\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "\n",
            "=== Текст 2 ===\n",
            "Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\n",
            "['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "\n",
            "=== Текст 3 ===\n",
            "I can't believe she's going! Let's meet at Jane's house. They'll love it.\n",
            "['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "\n",
            "=== Текст 4 ===\n",
            "What's the ETA for the package? Please e-mail support@example.com ASAP!\n",
            "['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e-mail', 'support', '@', 'example.com', 'ASAP', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишите функцию для токенизации текста с помощью Spacy"
      ],
      "metadata": {
        "id": "GxW7ZP6iqwpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy  # подключаем библиотеку spaCy\n",
        "\n",
        "# загружаем английскую модель \"en_core_web_sm\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def spacy_tokenization(string):\n",
        "    \"\"\"\n",
        "    Токенизация текста с помощью spaCy.\n",
        "    На вход: одна строка.\n",
        "    На выход: список токенов (слов и знаков препинания).\n",
        "    \"\"\"\n",
        "    doc = nlp(string)          # прогоняем строку через модель spaCy, получаем объект doc\n",
        "    return [t.text for t in doc]\n",
        "\n",
        "# ниже сразу применяем функцию ко всем предложениям из списка text\n",
        "for i, sentence in enumerate(text, start=1):\n",
        "    print(f\"\\n=== Текст {i} ===\")\n",
        "    print(sentence)\n",
        "    print(spacy_tokenization(sentence))"
      ],
      "metadata": {
        "id": "B0NQg-VfuFW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13ad1437-d073-41de-a33c-bbdd7aeed100"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Текст 1 ===\n",
            "The quick brown fox jumps over the lazy dog. It's a beautiful day!\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "\n",
            "=== Текст 2 ===\n",
            "Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\n",
            "['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "\n",
            "=== Текст 3 ===\n",
            "I can't believe she's going! Let's meet at Jane's house. They'll love it.\n",
            "['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "\n",
            "=== Текст 4 ===\n",
            "What's the ETA for the package? Please e-mail support@example.com ASAP!\n",
            "['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support@example.com', 'ASAP', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. С помощью цикла `for` примените каждую из написанных функций к каждому тексту из списка `texts`"
      ],
      "metadata": {
        "id": "WmyJfB9wuKkm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lvUmk94MhrL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "890c009e-0e8c-4d86-d6f2-be90b36e0e62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Текст 1 ===\n",
            "The quick brown fox jumps over the lazy dog. It's a beautiful day!\n",
            "\n",
            "simple_tokenization:\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'\", 's', 'a', 'beautiful', 'day', '!']\n",
            "\n",
            "nltk_tokenization:\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "\n",
            "spacy_tokenization:\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "\n",
            "=== Текст 2 ===\n",
            "Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\n",
            "\n",
            "simple_tokenization:\n",
            "['Dr', '.', 'Smith', 'arrived', 'at', '5', ':', '30', 'p', '.', 'm', '.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1', ',', '000', '.', '50', '.']\n",
            "\n",
            "nltk_tokenization:\n",
            "['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "\n",
            "spacy_tokenization:\n",
            "['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "\n",
            "=== Текст 3 ===\n",
            "I can't believe she's going! Let's meet at Jane's house. They'll love it.\n",
            "\n",
            "simple_tokenization:\n",
            "['I', 'can', \"'\", 't', 'believe', 'she', \"'\", 's', 'going', '!', 'Let', \"'\", 's', 'meet', 'at', 'Jane', \"'\", 's', 'house', '.', 'They', \"'\", 'll', 'love', 'it', '.']\n",
            "\n",
            "nltk_tokenization:\n",
            "['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "\n",
            "spacy_tokenization:\n",
            "['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "\n",
            "=== Текст 4 ===\n",
            "What's the ETA for the package? Please e-mail support@example.com ASAP!\n",
            "\n",
            "simple_tokenization:\n",
            "['What', \"'\", 's', 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support', '@', 'example', '.', 'com', 'ASAP', '!']\n",
            "\n",
            "nltk_tokenization:\n",
            "['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e-mail', 'support', '@', 'example.com', 'ASAP', '!']\n",
            "\n",
            "spacy_tokenization:\n",
            "['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support@example.com', 'ASAP', '!']\n"
          ]
        }
      ],
      "source": [
        "# для удобства просто переименуем text → texts\n",
        "texts = text\n",
        "\n",
        "# список функций, которые мы хотим применить\n",
        "functions = [\n",
        "    simple_tokenization,\n",
        "    nltk_tokenization,\n",
        "    spacy_tokenization,\n",
        "]\n",
        "\n",
        "# внешний цикл: идём по всем предложениям в списке texts\n",
        "for i, sentence in enumerate(texts, start=1):\n",
        "    print(f\"\\n=== Текст {i} ===\")\n",
        "    print(sentence)\n",
        "\n",
        "    # внутренний цикл: по очереди применяем КАЖДУЮ функцию к этому предложению\n",
        "    for func in functions:\n",
        "        # печатаем имя функции, чтобы было видно, какой способ токенизации используется\n",
        "        print(f\"\\n{func.__name__}:\")\n",
        "        # печатаем результат работы функции\n",
        "        print(func(sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqAgf6sGhrL8"
      },
      "source": [
        "##### Критерии оценки (макс. балл == 5):\n",
        "\n",
        "- Функциональность (до 4 баллов)): Все методы работают корректно (запускаем код, и он работает)\n",
        "- Качество кода (до 1 балла): Чистый, документированный код с обработкой ошибок (кратко описать, что вы дополнили самостоятельно, например, \"добавлена токенизация `spacy`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теоретические вопросы (макс. балл == 5; в ведомость выставляется сумма за практику и теорию)\n",
        "\n",
        "Необходимо дать краткие ответы на вопросы по теме \"токенизация\". В сумме длина ответов на вопрос не должна превышать размер вордовской страницы 14 шрифтом."
      ],
      "metadata": {
        "id": "Mwe1Co6MvibX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Почему простое разделение текста по пробелам и знакам препинания часто является недостаточным для современных NLP-задач? Приведите 2-3 конкретных примера, когда деление текста по символам или словам не работает. (2 балла за полный и подробный ответ на вопрос)\n",
        "\n",
        "2. Сколько токенов во фразе \"You shall know a word by the company it keeps\" в модели GPT-5? Как вы получили это значение? (1 балл за правильный ответ и ссылку на ресурс, с помощью которого вы узнали эту информацию)\n",
        "\n",
        "3. Опишите своими словами работу алгоритма BPE (можно форматировать ответ с использованием списков, 2 балла за корректное описание и ясное изложение ответа)"
      ],
      "metadata": {
        "id": "mgE2bQFXv0MG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Простое разделение по пробелам и знакам препинания (типa .split()) просто режет строку на кусочки по символам, что является недостаточным для соврменных NLP-задач. Такие методы не улавливают семантическое единство какой-то составной языковой единицы, что может привести к к проблемам в машинном переводе, анализе тональности и пр. Например, в предложенных строках \"I can't believe she's going!\" и \"What's the ETA for the package?\" присутствуют сокращения типо \"can't\", \"she's\" и \"what's\", которые важны для задач машинного перевода. Простое деление по пробелам и знакам препинания не считывает такие структуры как единые грамматические, которые влияют на смысл единицы и всего высказывания в целом. Это приводит к утрате грамматики и смысла. Также еще один пример – сложные единицы или сущности, которые должны уметь вычленяться в диалоге между ботом и человеком. \"5:30 p.m\" в \"Dr. Smith arrived at 5:30 p.m\", \"support@example.com\" в \"Please e-mail support@example.com ASAP!\" и др. при наивной токенизации сломаются, превратившись в простой набор чисел и мусорных знаков. Нам важно воспринимать их целыми и логичными токеном, чтобы правильно извлекать факты/информацию из текстов.\n",
        "2. Во фразе \"You shall know a word by the company it keeps\" без кавычек 10 токенов в модели GPT-5. Ссылка на ресурс, где узнал информацию, - https://gpt-tokenizer.dev/\n",
        "3. Главная идея научить модель работать не только со словами, но и с их частями. BPE модель автоматически учится таким кусочкам на корпусе. В начале считаем каждый символ отдельным токеном, далее смотрим какие пары соседних символов чаще всего в обучающем тексте. Самые частотные пары мы объединяем в новый токен и повторяем это много раз да одном большом корпусе. Мы продолжаем делать это пока не наберем нужный размер словаря. Когда мы встречаем новое слово то разбиваем его уже на части, которые есть в словаре. Следовательно, BPE учится подсловными токенами на корпусе, начиная с отдельных символов, на каждом шаге склеивая самые частотные пары соседних токенов и в результате получая словарь подслов, с помощью которого можно токенизировать новые слова. Модель становится устойчивой и чувтсительной к морфологии."
      ],
      "metadata": {
        "id": "OZS5RBm8egyd"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}